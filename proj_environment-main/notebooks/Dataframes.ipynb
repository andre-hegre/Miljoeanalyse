{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Her skal vi undersøke dataen hentet og bruke Pandas/SQL for å forstå strukturen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Her begynner vi med å forstå dataen fra frost bedre, og gjøre det om til en pandas dataframe, slik at det blir lett å jobbe med videre. \n",
    "\n",
    "I tillegg skal vi se på hvordan vi kan bruke pandasql (sqldf) for å gjøre querys på dataen og filtrere gjennom lettvint. Vi begynner med å hente file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import os\n",
    "\n",
    "# Henter file path\n",
    "\n",
    "notebook_directory = os.getcwd()\n",
    "root = os.path.abspath(os.path.join(notebook_directory, \"..\"))\n",
    "file_path = os.path.join(root, \"data\", \"Frost_Observations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataen vi hentet er laget med mange dictionaries, som igjen inneholder flere dictionaries. For å kun få ut relevant data kan vi filtrere gjennom python for enkelte \"entries\" i forskjellige dictionaries. Deretter kan vi gjøre det om til en dataframe ved bruk av pandas og se hvordan dataen ser ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(file_path,\"r\") as fil: #leser filen og lagrer den som \"data\"\n",
    "    data=json.load(fil)\n",
    "\n",
    "# filtrerer dataen for bare det som er relevant til å analysere\n",
    "brukbar_data = [\n",
    "    {\n",
    "        \"elementId\": obs[\"elementId\"],\n",
    "        \"sourceId\": entry[\"sourceId\"],\n",
    "        \"referenceTime\": entry[\"referenceTime\"],\n",
    "        \"value\": obs[\"value\"],\n",
    "        \"unit\": obs[\"unit\"]\n",
    "    }\n",
    "    for entry in data[\"data\"]\n",
    "    for obs in entry[\"observations\"] \n",
    "]\n",
    "\n",
    "df=pd.DataFrame(brukbar_data) #Dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None) #justerer output display\n",
    "pd.set_option('display.width', 1000)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da har vi fått lagd en dataframe med pandas som viser relevant data. Det ser litt uryddig ut og det passer bedre å ha hver observasjon som kolonner og så verdiene nedover, så vi kan bruke pd.pivot til til å gjøre det mer ryddig. \n",
    "\n",
    "Men det at vi har 2 verdier for mean(air_temperature P1D) og for sum(precipitation_amount P1D) gjør at dette ikke fungerer. Foreløpig kan vi kombinere det ved å ta gjennomsnittsverdien av de 2 målte, slik at vi kan visualisere dataframen bedre. Vi kan bruke \"groupby\" funksjonen for å samle de 2 identiske elementene og slå sammen verdiene med gjennomsnittet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rader=[]\n",
    "\n",
    "# henter relevant data ved å iterere gjennom i forskjellige dictionaries\n",
    "for entry in data[\"data\"]:\n",
    "    referenceTime = entry[\"referenceTime\"]\n",
    "    for obs in entry[\"observations\"]:\n",
    "         rader.append({\n",
    "            \"referenceTime\": referenceTime,\n",
    "            \"elementId\": obs[\"elementId\"],\n",
    "            \"value\": obs[\"value\"],\n",
    "        })\n",
    "         \n",
    "df=pd.DataFrame(rader) #dataframe\n",
    "\n",
    "# slår sammen der det er 2 av mean(air_temperature P1D) og tar gjennomsnittet, \n",
    "# gjør pivot av dataframe, slik at kolonnene blir type observasjon\n",
    "\n",
    "df_gruppering = df.groupby([\"referenceTime\", \"elementId\"], as_index=False)[\"value\"].mean()\n",
    "df2=df_gruppering.pivot(index=\"referenceTime\",columns=\"elementId\",values=\"value\")\n",
    "df2=df2.reset_index()\n",
    "print(df2.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagrer df2 som json fil for videre bruk\n",
    "df2.to_json('../data/df2_data.json',indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ser vi dataen på en mer forståelig måte. Det er verdt å merke at \"boolean_clear_sky_weather(cloud_area_fraction P1D)\" er boolean verdier, så i datahåndteringen kan vi ta hensyn til det og omgjøre det fra float. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruker PANDAS SQL (sqldf) til å hente data fra DataFrame\n",
    "\n",
    "Nå som vi har en fungerende DataFrame, kan vi hente diverse type data og spesifikk data ved bruk av Pandas SQL/sqldf: \n",
    "\n",
    "1) Gjennomsnittlig temperatur for Juli i årene tilgjengelig\n",
    "2) Alle dager der 24 timers gjennomsnittstemperaturen er over 20 grader og der det ikke var skyete basert på \"boolean_clear_sky_weather...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "    SELECT strftime('%Y',referenceTime) AS År,\n",
    "    AVG(\"mean(air_temperature P1D)\") AS avg_temperatur_juli\n",
    "    FROM df2\n",
    "    WHERE strftime('%m',referenceTime) = '07'\n",
    "    GROUP BY År\n",
    "\"\"\"\n",
    "resultat=pysqldf(query)\n",
    "print(resultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her har vi for hele tidsperioden vi har i flere år:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT strftime('%Y-%m', referenceTime) AS Month, \n",
    "           AVG(\"mean(air_temperature P1D)\") AS avg_temperature\n",
    "    FROM df2\n",
    "    GROUP BY Month\n",
    "    ORDER BY Month;\n",
    "\"\"\"\n",
    "result = sqldf(query, globals())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2=\"\"\"\n",
    "    SELECT * FROM df2\n",
    "    WHERE \"mean(air_temperature P1D)\">=20.0\n",
    "    ORDER BY \"mean(air_temperature P1D)\" DESC;\n",
    "\"\"\"\n",
    "resultat=pysqldf(query2)\n",
    "print(resultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omgjøring av luftkvalitet-data til panda dataframe + forståelse: \n",
    "\n",
    "Her skal vi sortere gjennom luftkvalitetsdataen. Vi henter file-path og gjør om til data frame med pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import os\n",
    "\n",
    "# Henter file path\n",
    "\n",
    "notebook_directory = os.getcwd()\n",
    "root = os.path.abspath(os.path.join(notebook_directory, \"..\"))\n",
    "file_path_L = os.path.join(root, \"data\", \"luftkvalitet_Kirkeveien_all_years.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Først kan vi hente generell informasjon om det som ligger i filen, og lager en dataframe basert på geneerell info, som stasjon, område, hva vi måler og id: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(file_path_L,\"r\") as fil: #leser filen og lagrer den som \"data\"\n",
    "    luftdata=json.load(fil)\n",
    "\n",
    "general_data = []\n",
    "for entry in luftdata:\n",
    "    # Extract general information\n",
    "    general_info = {\n",
    "        \"id\": entry.get(\"id\"),\n",
    "        \"zone\": entry.get(\"zone\"),\n",
    "        \"municipality\": entry.get(\"municipality\"),\n",
    "        \"area\": entry.get(\"area\"),\n",
    "        \"station\": entry.get(\"station\"),\n",
    "        \"component\": entry.get(\"component\"),\n",
    "        \"unit\": entry.get(\"unit\"),\n",
    "        \"latitude\": entry.get(\"latitude\"),\n",
    "        \"longitude\": entry.get(\"longitude\"),\n",
    "    }\n",
    "    \n",
    "    # Extract values (nested list of dictionaries)\n",
    "    for value in entry.get(\"values\", []):\n",
    "        # Combine general info with each value\n",
    "        row = general_info.copy()\n",
    "        row.update(value)  # Add fields from the \"values\" dictionary\n",
    "        general_data.append(row)\n",
    "\n",
    "# Create a DataFrame\n",
    "df1 = pd.DataFrame(general_data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for entry in luftdata:\n",
    "    component = entry.get(\"component\")  # e.g., NO2, PM10, etc.\n",
    "    for value_entry in entry.get(\"values\", []):\n",
    "        # Extract date and value\n",
    "        processed_data.append({\n",
    "            \"date\": value_entry.get(\"dateTime\"),  # Assuming \"fromTime\" is the date field\n",
    "            \"component\": component,\n",
    "            \"value\": value_entry.get(\"value\")  # Assuming \"value\" is the observed value\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Pivot the DataFrame to have components as columns\n",
    "pivot_df = df.pivot(index=\"date\", columns=\"component\", values=\"value\")\n",
    "\n",
    "# Reset the index to make it easier to work with\n",
    "pivot_df.reset_index(inplace=True)\n",
    "pivot_df[\"date\"] = pd.to_datetime(pivot_df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "print(pivot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagrer filen som \"df1_data.json\"\n",
    "\n",
    "pivot_df.to_json('../data/df1_data.json',indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her er et eksempel på bruk av sqldf for å hente data fra dataframe om luftkvalitet: \n",
    "\n",
    "- Total gjennomsnittlig verdi for de forskjellige partikkelmålingene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT \n",
    "        AVG(\"NO2\") AS avg_NO2,\n",
    "        AVG(\"PM10\") AS avg_PM10,\n",
    "        AVG(\"PM2.5\") AS avg_PM25,\n",
    "        AVG(\"CO\") AS avg_CO,\n",
    "        AVG(\"NO\") AS avg_NO,\n",
    "        AVG(\"NOx\") AS avg_NOx\n",
    "    FROM pivot_df\n",
    "\"\"\"\n",
    "result = pysqldf(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
